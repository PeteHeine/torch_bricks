{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchBricks\n",
    "\n",
    "[![codecov](https://codecov.io/gh/PeteHeine/torchbricks/branch/main/graph/badge.svg?token=torchbricks_token_here)](https://codecov.io/gh/PeteHeine/torchbricks)\n",
    "[![CI](https://github.com/PeteHeine/torchbricks/actions/workflows/main.yml/badge.svg)](https://github.com/PeteHeine/torchbricks/actions/workflows/main.yml)\n",
    "\n",
    "TorchBricks builds pytorch models using small reuseable and decoupled parts - we call them bricks.\n",
    "\n",
    "The concept is simple and flexible and allows you to more easily combine and swap out parts of the model (preprocessor, backbone, neck, head or post-processor), change the task or extend it with multiple tasks.\n",
    "\n",
    "## Basic use-case: Image classification\n",
    "Let us see it in action:\n",
    "\n",
    "First we specify regular pytorch modules: A preprocessor, a model and a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class PreprocessorDummy(nn.Module):\n",
    "    def forward(self, raw_input: torch.Tensor) -> torch.Tensor:\n",
    "        return raw_input/2\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_features, 1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(tensor)\n",
    "\n",
    "class ClassifierDummy(nn.Module):\n",
    "    def __init__(self, num_classes: int, in_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(torch.flatten(self.avgpool(tensor)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use torchbricks to define how the modules are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['raw', 'phase', 'processed', 'embedding', 'logits'])\n"
     ]
    }
   ],
   "source": [
    "from torchbricks.bricks import BrickCollection, BrickNotTrainable, BrickTrainable, Phase\n",
    "\n",
    "# Defining model from bricks\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), input_names=['raw'], output_names=['processed']),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=3, n_features=10), input_names=['processed'], output_names=['embedding']),\n",
    "    'image_classifier': BrickTrainable(ClassifierDummy(num_classes=3, in_features=10), input_names=['embedding'], output_names=['logits'])\n",
    "}\n",
    "\n",
    "# Executing model\n",
    "model = BrickCollection(bricks)\n",
    "batch_image_example = torch.rand((1, 3, 100, 200))\n",
    "outputs = model(named_inputs={'raw': batch_image_example}, phase=Phase.TRAIN)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All modules are added as entries in a regular dictionary, and for each module we 1) specify a name \n",
    "2) if it is trainable or not (`BrickTrainable`/`BrickNotTrainable`) and 3) input and output names. \n",
    "   \n",
    "Finally, bricks are collected in a `BrickCollection`. A `BrickCollection` has the functionality of a \n",
    "regular `nn.Module` with a `forward`-function, `to` to move to a specific device/precision, \n",
    "save/loading and management of parameters etc.\n",
    "\n",
    "Beyond that, the brick collections acts as a simple DAG, it accepts a dictionary (`named_inputs`), \n",
    "executes each bricks and ensures that the output of one brick is passed to the inputs of other bricks with matching names. \n",
    "\n",
    "Note also that we set `phase=Phase.TRAIN` to explicitly specify if we are doing training, validation, test or inference.\n",
    "Specifying a phase is important, if we want a module to act in a specific way during specific phases.\n",
    "We will get back to this later. \n",
    "\n",
    "Not show here, a `BrickCollection` also supports a nested dictionary of bricks. A nested brick collections acts the same, \n",
    "but it becomes easier to add and remove sub-collections bricks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of bricks - reusable bricks modules\n",
    "We provide a bag-of-bricks with commonly used `nn.Module`s \n",
    "\n",
    "Below we create a brick collection with a real world example including a `Preprocessor`, an adaptor function to convert\n",
    "torchvision resnet models into a backbone brick with no classifier and an `ImageClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessor': BrickNotTrainable(),\n",
       " 'backbone': BrickTrainable(\n",
       "   (model): BackboneResnet(\n",
       "     (resnet): ResNet(\n",
       "       (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "       (layer1): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (layer2): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (downsample): Sequential(\n",
       "             (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "             (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (layer3): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (downsample): Sequential(\n",
       "             (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "             (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (layer4): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (downsample): Sequential(\n",
       "             (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "             (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "       (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'image_classifier': BrickTrainable(\n",
       "   (model): ImageClassifier(\n",
       "     (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "     (softmax): Softmax(dim=1)\n",
       "     (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "from torchbricks.bag_of_bricks import ImageClassifier, resnet_to_brick, Preprocessor\n",
    "\n",
    "num_classes = 10\n",
    "resnet_brick = resnet_to_brick(resnet=resnet18(weights=False, num_classes=num_classes),  input_name='normalized', output_name='features')\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), input_names=['raw'], output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'image_classifier': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=resnet_brick.model.n_backbone_features),\n",
    "                                     input_names=['features'], output_names=['logits', 'probabilities', 'class_prediction']),\n",
    "}\n",
    "bricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case: Bricks `on_step`-function for training and evaluation\n",
    "In above examples, we have showed how to compose trainable and non-trainable bricks, and how a dictionary of tensors is passed\n",
    "to the forward function... But TorchBricks goes beyond that.\n",
    "\n",
    "An important feature of a brick collection is the `on_step`-function to also calculate metrics and losses.\n",
    "\n",
    "We will extend the example from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m named_outputs, losses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mon_step(phase\u001b[39m=\u001b[39mPhase\u001b[39m.\u001b[39mTRAIN, named_inputs\u001b[39m=\u001b[39mnamed_inputs, batch_idx\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m named_outputs, losses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mon_step(phase\u001b[39m=\u001b[39mPhase\u001b[39m.\u001b[39mTRAIN, named_inputs\u001b[39m=\u001b[39mnamed_inputs, batch_idx\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msummarize(phase\u001b[39m=\u001b[39;49mPhase\u001b[39m.\u001b[39;49mTRAIN, reset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/code/torchbricks/src/torchbricks/bricks.py:95\u001b[0m, in \u001b[0;36mBrickCollection.summarize\u001b[0;34m(self, phase, reset)\u001b[0m\n\u001b[1;32m     93\u001b[0m metrics \u001b[39m=\u001b[39m {}\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m brick \u001b[39min\u001b[39;00m update_metrics_bricks:\n\u001b[0;32m---> 95\u001b[0m     metrics\u001b[39m.\u001b[39;49mupdate(brick\u001b[39m.\u001b[39;49msummarize(phase\u001b[39m=\u001b[39;49mphase, reset\u001b[39m=\u001b[39;49mreset))\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/mambaforge/envs/torchbricks/lib/python3.10/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[39m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[39m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[39m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39miteration over a 0-d tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt change the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "from torchbricks.bricks import BrickLoss, BrickTorchMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "\n",
    "bricks[\"accuracy\"] = BrickTorchMetric(MulticlassAccuracy(num_classes=num_classes), input_names=['class_prediction', 'targets'])\n",
    "bricks[\"loss\"] = BrickLoss(model=nn.CrossEntropyLoss(), input_names=['logits', 'targets'], output_names=['loss_ce'])\n",
    "\n",
    "\n",
    "# We can still run the forward-pass as before - Note: The forward call does not require 'targets'\n",
    "model = BrickCollection(bricks)\n",
    "batch_image_example = torch.rand((1, 3, 100, 200))\n",
    "outputs = model(named_inputs={\"raw\": batch_image_example}, phase=Phase.TRAIN)\n",
    "\n",
    "# Example of running `on_step`. Note: `on_step` requires `targets` to calculate metrics and loss.\n",
    "named_inputs = {\"raw\": batch_image_example, \"targets\": torch.ones((1), dtype=torch.int64)}\n",
    "named_outputs, losses = model.on_step(phase=Phase.TRAIN, named_inputs=named_inputs, batch_idx=0)\n",
    "named_outputs, losses = model.on_step(phase=Phase.TRAIN, named_inputs=named_inputs, batch_idx=1)\n",
    "named_outputs, losses = model.on_step(phase=Phase.TRAIN, named_inputs=named_inputs, batch_idx=2)\n",
    "metrics = model.summarize(phase=Phase.TRAIN, reset=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic use-case: Semantic Segmentation\n",
    "After running experiments, we now realize that we also wanna do semantic segmentation.\n",
    "This is how it would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_implementation = True\n",
    "if missing_implementation:\n",
    "    print(\"MISSING\")\n",
    "else:\n",
    "    # We can optionally keep/remove image_classification from before\n",
    "    bricks.pop(\"image_classifier\")\n",
    "\n",
    "    # Add upscaling and semantic segmentation nn.Modules\n",
    "    bricks[\"upscaling\"] = BrickTrainable(Upscaling(), input_names=[\"embedding\"], output_names=[\"embedding_upscaled\"])\n",
    "    bricks[\"semantic_segmentation\"] = BrickTrainable(SegmentationClassifier(), input_names=[\"embedding_upscaled\"], output_names=[\"ss_logits\"])\n",
    "\n",
    "    # Executing model\n",
    "    model = BrickCollection(bricks)\n",
    "    batch_image_example = torch.rand((1, 3, 100, 200))\n",
    "    outputs = model(named_inputs={\"raw\": batch_image_example}, phase=Phase.TRAIN)\n",
    "\n",
    "    print(outputs.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By wrapping both core model computations, metrics and loss functions into a single brick collection, we can more easily swap between\n",
    "running model experiments in notebooks, trainings\n",
    "\n",
    "We provide a `forward` function to easily run model inference without targets and an `on_step` function\n",
    "to easily get metrics and losses in both"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case: Training with a collections of bricks\n",
    "We like and love pytorch-lightning! We can avoid writing the easy-to-get-wrong training loop, write validation/test scrips, it create\n",
    "logs, ensures training is done efficiently on any device (CPU, GPU, TPU), on multiple devices with reduced precision and much more.\n",
    "\n",
    "But with pytorch-lightning you need to specify a LightningModule and I find myself hiding the important stuff in the class\n",
    "and using multiple levels of inheritance. It can make your code unnecessarily complicated, hard to read and hard to reuse.\n",
    "It may also require some heavy refactoring changing to a new task or switching to multiple tasks.\n",
    "\n",
    "With a brick collection you should rarely change or inherit your lightning module, instead you inject the model, metrics and loss functions\n",
    "into a lightning module. Changes to preprocessor, backbone, necks, heads, metrics and losses are done on the outside\n",
    "and injected into the lightning module.\n",
    "\n",
    "Below is an example of how you could inject our brick collection into our custom `LightningBrickCollection`.\n",
    "The brick collection can be image classification, semantic segmentation, object detection or all of them at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_opimtizer_func = partial(torch.optim.SGD, lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "bricks_lightning_module = LightningBrickCollection(path_experiments=path_experiments,\n",
    "                                                   experiment_name=experiment_name,\n",
    "                                                   brick_collection=brick_collection,\n",
    "                                                   create_optimizer_func=create_opimtizer_func)\n",
    "\n",
    "logger = WandbLogger(name=experiment_name, project=PROJECT)\n",
    "trainer = Trainer(accelerator=args.accelerator, logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(bricks_lightning_module,\n",
    "            train_dataloaders=data_module.train_dataloader(),\n",
    "            val_dataloaders=data_module.val_dataloader())\n",
    "trainer.test(bricks_lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested brick collections\n",
    "It can handle nested brick collections and nested dictionary of bricks.\n",
    "\n",
    "MISSING\n",
    "\n",
    "## TorchMetric.MetricCollection\n",
    "\n",
    "MISSING\n",
    "\n",
    "## Why should I explicitly set the train, val or test phase\n",
    "\n",
    "MISSING\n",
    "\n",
    "##\n",
    "\n",
    "## What are we missing?\n",
    "\n",
    "\n",
    "- [ ] Proper `LightningBrickCollection` for other people to use\n",
    "- [ ] Collection of helper modules. Preprocessors, Backbones, Necks/Upsamplers, ImageClassification, SemanticSegmentation, ObjectDetection\n",
    "  - [ ] All the modules in the README should be easy to import as actually modules.\n",
    "  - [ ] Make common brick collections: BricksImageClassification, BricksSegmentation, BricksPointDetection, BricksObjectDetection\n",
    "- [ ] Support preparing data in the dataloader?\n",
    "- [ ] Make common Visualizations with pillow - not opencv to not blow up the required dependencies. ImageClassification, Segmentation, ObjectDetection\n",
    "- [ ] Make an export to onnx function and add it to the README.md\n",
    "- [ ] Proper handling of train, val and test. What to do with gradients, nn.Module parameters and internal eval/train state\n",
    "- [ ] Consider: If train, val and test phase has no impact on bricks, it should be passed as a regular named input.\n",
    "- [x] Minor: BrickCollections supports passing a dictionary with BrickCollections. But we should also convert a nested dictionary into a nested brick collections\n",
    "- [x] Minor: Currently, `input_names` and `output_names` support positional arguments, but we should also support keyword arguments.\n",
    "- [x] Minor: Make Brick an abstract class\n",
    "- [x] Convert torchvision resnet models to only a backbone brick.\n",
    "- [ ] Make readme a notebook\n",
    "- [ ] Ensure that all examples in the `README.md` are working with easy to use modules. \n",
    "- [ ] Test: Make it optional if gradients can be passed through NonTrainableBrick without weights being optimized\n",
    "\n",
    "\n",
    "## How does it really work?\n",
    "????\n",
    "\n",
    "## Install it from PyPI\n",
    "\n",
    "```bash\n",
    "pip install torchbricks\n",
    "```\n",
    "\n",
    "## Development\n",
    "\n",
    "Read the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n",
    "\n",
    "### Install\n",
    "\n",
    "    conda create --name torchbricks --file conda-linux-64.lock\n",
    "    conda activate torchbricks\n",
    "    poetry install\n",
    "\n",
    "### Activating the environment\n",
    "\n",
    "    conda activate torchbricks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
