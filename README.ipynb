{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchBricks\n",
    "[![codecov](https://codecov.io/gh/pete-machine/torchbricks/branch/main/graph/badge.svg?token=torchbricks_token_here)](https://codecov.io/gh/pete-machine/torchbricks)\n",
    "[![CI](https://github.com/pete-machine/torchbricks/actions/workflows/main.yml/badge.svg)](https://github.com/pete-machine/torchbricks/actions/workflows/main.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchBricks builds pytorch models using small reuseable and decoupled parts - we call them bricks. \n",
    "\n",
    "The concept is simple and flexible and allows you to more easily combine, add or swap out parts of the model \n",
    "(preprocessor, backbone, neck, head or post-processor), change the task or extend it with multiple tasks.\n",
    "\n",
    "TorchBricks also serves as a compact recipe on both how model parts are connected and when parts are executed \n",
    "during model stages such as training, validation, testing, inference, export and potentially other stages. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Install it with pip\n",
    "\n",
    "```bash\n",
    "pip install torchbricks\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricks by example\n",
    "\n",
    "To demonstrate the the concepts of TorchBricks, we will first specify some dummy parts used of a regular image recognition model: \n",
    "A preprocessor, a backbone and a head (in this case a classifier).\n",
    "*Note: Don't worry about the actually implementation of these modules -they are just dummy examples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "import torch\n",
    "from torch import nn\n",
    "class PreprocessorDummy(nn.Module):\n",
    "    def forward(self, raw_input: torch.Tensor) -> torch.Tensor:\n",
    "        return raw_input/2\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(tensor)\n",
    "\n",
    "class ClassifierDummy(nn.Module):\n",
    "    def __init__(self, num_classes: int, in_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.fc(torch.flatten(self.avgpool(tensor), start_dim = 1))\n",
    "        return logits, self.softmax(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concept 1: Bricks are connected\n",
    "An important concept of TorchBricks is that it defines how modules are connected by specifying input and output names of\n",
    "each module similar to a DAG. \n",
    "\n",
    "In below code snippet, we demonstrate how this would look for our dummy model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchbricks.bricks import BrickCollection, BrickTrainable, BrickNotTrainable, BrickLoss\n",
    "from torchbricks.bricks import Stage\n",
    "from torchbricks.graph_plotter import create_mermaid_dag_graph\n",
    "\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), \n",
    "                                      input_names=['raw'], \n",
    "                                      output_names=['processed']),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=3, n_features=10), \n",
    "                               input_names=['processed'], \n",
    "                               output_names=['embedding']),\n",
    "    'head': BrickTrainable(ClassifierDummy(num_classes=3, in_features=10), \n",
    "                           input_names=['embedding'], \n",
    "                           output_names=['logits', \"softmaxed\"]),\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "print(brick_collection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each module is placed in a dictionary with a unique name and wrapped inside a brick with input and output names. \n",
    "Input and output names specifies how outputs of one module is passed to inputs of the next module. \n",
    "\n",
    "Finally, the dictionary of bricks is passed to a `BrickCollection`. \n",
    "\n",
    "Below we visualize how the brick collection connects the different bricks together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(\"<strong>BrickNotTrainable</strong><br><strong>preprocessor</strong>: PreprocessorDummy\"):::BrickNotTrainable\n",
    "    backbone(\"<strong>BrickTrainable</strong><br><strong>backbone</strong>: TinyModel\"):::BrickTrainable\n",
    "    head(\"<strong>BrickTrainable</strong><br><strong>head</strong>: ClassifierDummy\"):::BrickTrainable\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw:::input --> preprocessor\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |processed| backbone\n",
    "    backbone --> |embedding| head\n",
    "    head --> softmaxed:::output\n",
    "    head --> logits:::output\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```\n",
    "*Graph is visualized using [mermaid](https://github.com/mermaid-js/mermaid) syntax.*\n",
    "*We provide the `create_mermaid_dag_graph`-function to create a brick collection visualization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BrickCollection` is used for executing above graph using a dictionary as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "batch_images = torch.rand((batch_size, 3, 100, 200))\n",
    "named_outputs = brick_collection(named_inputs={'raw': batch_images}, stage=Stage.INFERENCE)\n",
    "print(\"Brick outputs:\", named_outputs.keys())\n",
    "# Brick outputs: dict_keys(['raw', 'stage', 'processed', 'embedding', 'logits', 'softmaxed'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brick collection accepts a dictionary and returns a dictionary with all intermediated and resulting tensors. \n",
    "\n",
    "Running our models as a brick collection has the following advantages:\n",
    "\n",
    "- A brick collection act as a regular `nn.Module` with all the familiar features: a `forward`-function, a `to`-function to move \n",
    "  to a specific device/precision, you can save/load a model, management of parameters, onnx exportable etc. \n",
    "- A brick collection is also a simple DAG, it accepts a dictionary with \"named data\" (we call this `named_inputs`), \n",
    "executes each bricks and ensures that the outputs are passed to the inputs of other bricks with matching names. \n",
    "Structuring the model as a DAG, makes it easy to add/remove outputs for a given module during development, add new modules to the\n",
    "collection and build completely new models from reusable parts. \n",
    "- A brick collection is actually a dictionary (`nn.DictModule`). Allowing you to access, pop and update the \n",
    "  collection easily as a regular dictionary. It can also handle nested dictionary, allowing groups of bricks to be added/removed easily. \n",
    "\n",
    "Note also that we set `stage=Stage.INFERENCE` to explicitly specify if we are doing training, validation, test or inference.\n",
    "Specifying a stage is important, if we want a module to act in a specific way during a specific stages.\n",
    "\n",
    "Leading us to the next section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept 2: Bricks can be dead (or alive)\n",
    "The second concept is to specify when bricks are alive - meaning we specify at which stages (train, test, validation, inference and export) \n",
    "a brick is active. \n",
    "\n",
    "For other stage the brick will play dead - do nothing / return an empty dictionary. \n",
    "\n",
    "Meaning that for different `stages`, we will have the option of creating a unique DAG for each model stage. \n",
    "\n",
    "In above example this is not particular interesting - because preprocessor, backbone model and head would typically be alive in all stages. \n",
    "\n",
    "So we will demonstrate by adding a loss brick (`BrickLoss`) and specifying `alive_stages` for each brick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), \n",
    "                                      input_names=['raw'], \n",
    "                                      output_names=['processed'], \n",
    "                                      alive_stages=\"all\"),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=num_classes, n_features=10), \n",
    "                               input_names=['processed'], \n",
    "                               output_names=['embedding'], \n",
    "                               alive_stages=\"all\"),\n",
    "    'head': BrickTrainable(ClassifierDummy(num_classes=num_classes, in_features=10), \n",
    "                           input_names=['embedding'], \n",
    "                           output_names=['logits', 'softmaxed'], \n",
    "                           alive_stages=\"all\"),\n",
    "    'loss': BrickLoss(model=nn.CrossEntropyLoss(), \n",
    "                      input_names=['logits', 'targets'], \n",
    "                      output_names=['loss_ce'], \n",
    "                      alive_stages=[Stage.TRAIN, Stage.VALIDATION, Stage.TEST])\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "\n",
    "print(brick_collection)\n",
    "# BrickCollection(\n",
    "#   (preprocessor): BrickNotTrainable(PreprocessorDummy, input_names=['raw'], output_names=['processed'], alive_stages=['TRAIN', 'VALIDATION', 'TEST', 'INFERENCE', 'EXPORT'])\n",
    "#   (backbone): BrickTrainable(TinyModel, input_names=['processed'], output_names=['embedding'], alive_stages=['TRAIN', 'VALIDATION', 'TEST', 'INFERENCE', 'EXPORT'])\n",
    "#   (head): BrickTrainable(ClassifierDummy, input_names=['embedding'], output_names=['logits', 'softmaxed'], alive_stages=['TRAIN', 'VALIDATION', 'TEST', 'INFERENCE', 'EXPORT'])\n",
    "#   (loss): BrickLoss(CrossEntropyLoss, input_names=['logits', 'targets'], output_names=['loss_ce'], alive_stages=['TRAIN', 'VALIDATION', 'TEST'])\n",
    "# )\n",
    "print(create_mermaid_dag_graph(brick_collection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set `preprocessor`, `backbone` and `head` to be alive on all stages `alive_stages=\"all\"` - this is the default behavior and\n",
    "similar to before. \n",
    " \n",
    "For `loss` we set `alive_stages=[Stage.TRAIN, Stage.VALIDATION, Stage.TEST]` to only calculate loss during train, validation and test\n",
    "stages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Graph during inference and export:**\n",
    "During `Stage.INFERENCE` and `Stage.EXPORT`, the loss modules is dead and note only `raw` is required as input:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(\"<strong>BrickNotTrainable</strong><br><strong>preprocessor</strong>: PreprocessorDummy\"):::BrickNotTrainable\n",
    "    backbone(\"<strong>BrickTrainable</strong><br><strong>backbone</strong>: TinyModel\"):::BrickTrainable\n",
    "    head(\"<strong>BrickTrainable</strong><br><strong>head</strong>: ClassifierDummy\"):::BrickTrainable\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw:::input --> preprocessor\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |processed| backbone\n",
    "    backbone --> |embedding| head\n",
    "    head --> softmaxed:::output\n",
    "    head --> logits:::output\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "        \n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph during train, test and validation:**\n",
    "\n",
    "During `Stage.TRAIN`, `Stage.VALIDATION` and `Stage.TEST`, the loss module is alive and note both `raw` and `targets` are required as inputs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(\"<strong>BrickNotTrainable</strong><br><strong>preprocessor</strong>: PreprocessorDummy\"):::BrickNotTrainable\n",
    "    backbone(\"<strong>BrickTrainable</strong><br><strong>backbone</strong>: TinyModel\"):::BrickTrainable\n",
    "    head(\"<strong>BrickTrainable</strong><br><strong>head</strong>: ClassifierDummy\"):::BrickTrainable\n",
    "    loss(\"<strong>BrickLoss</strong><br><strong>loss</strong>: CrossEntropyLoss\"):::BrickLoss\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw:::input --> preprocessor\n",
    "    targets:::input --> loss\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |processed| backbone\n",
    "    backbone --> |embedding| head\n",
    "    head --> |logits| loss\n",
    "    head --> softmaxed:::output\n",
    "    loss --> loss_ce:::output\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    classDef BrickLoss stroke-width:0px,fill:#5C677D \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricks for model training\n",
    "**We are not creating a training framework**, but to easily use the brick collection in your favorite training framework or custom \n",
    "training/validation/test loop, we need the final piece: **Calculating model metrics** \n",
    "\n",
    "To easily inject both model, losses and metrics, we also need to easily support metrics and calculate metrics across a dataset. \n",
    "We will extend our example from before by adding metric bricks. \n",
    "\n",
    "To calculate metrics across a dataset, we heavily rely on concepts and functions used in the \n",
    "[TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/) library.\n",
    "\n",
    "The used of TorchMetrics in a brick collection is demonstrated in below code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchbricks.bag_of_bricks import ImageClassifier, Preprocessor, resnet_to_brick\n",
    "from torchbricks.bricks import BrickMetricSingle\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "num_classes = 10\n",
    "resnet = torchvision.models.resnet18(weights=None, num_classes=num_classes)\n",
    "resnet_brick = resnet_to_brick(resnet=resnet,  input_name='normalized', output_name='features')\n",
    "n_features = resnet_brick.model.n_backbone_features\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), \n",
    "                                      input_names=['raw'], \n",
    "                                      output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'head': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=n_features),\n",
    "                           input_names=['features'], \n",
    "                           output_names=['logits', 'probabilities', 'class_prediction']),\n",
    "    'accuracy': BrickMetricSingle(MulticlassAccuracy(num_classes=num_classes), \n",
    "                                  input_names=['class_prediction', 'targets']),\n",
    "    'loss': BrickLoss(model=nn.CrossEntropyLoss(), \n",
    "                      input_names=['logits', 'targets'], \n",
    "                      output_names=['loss_ce'])\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the brick collection above to simulate how a user can iterate over a dataset and \n",
    "calling the brick for each batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch in range(5): # Simulates iterating over the dataset\n",
    "    named_inputs = {\"raw\": batch_images, \"targets\": torch.ones((batch_size), dtype=torch.int64)}\n",
    "    named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.TRAIN)\n",
    "\n",
    "metrics = brick_collection.summarize(stage=Stage.TRAIN, reset=True)\n",
    "print(f\"{metrics=}, {named_outputs.keys()=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration in our (simulated) dataset, we calculate model outputs, losses and metrics for each batch. \n",
    "Unlike other bricks, `BrickMetrics` will not (by default) output metrics for each batch. \n",
    "Instead metrics are stored internally in `BrickMetricSingle` and only aggregated and return when\n",
    "the `summarize` function is called. In above example the aggregated metric over over 5 batches.\n",
    "\n",
    "It is important to note that we set `reset=True` to reset the internal aggregation of metrics.  \n",
    "\n",
    "**Additional notes on metrics**\n",
    "\n",
    "You have the option of either using a single metric (`torchmetrics.Metric`) with `BrickMetricSingle` or a collection of \n",
    "metrics (`torchmetrics.MetricCollection`) with `BrickMetrics`.\n",
    "\n",
    "For multiple metrics, we advice to use `BrickMetrics` with a `torchmetrics.MetricCollection` \n",
    "[doc](https://torchmetrics.readthedocs.io/en/stable/pages/overview.html#metriccollection). \n",
    "It has some intelligent mechanisms for efficiently sharing calculation for multiple metrics.\n",
    "\n",
    "Note also that metrics are not passed to other bricks or returned as output of the brick collection - they are only stored internally. \n",
    "To also pass metrics to other bricks, you can set `return_metrics=True` for `BrickMetrics` and `BrickMetricSingle`. \n",
    "But be aware, this will add computational cost. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricks motivation (to be continued)\n",
    "\n",
    "The main motivation:\n",
    "- \n",
    "- Each brick can return whatever - they are not forced to only returning e.g. logits... If you want the model backbone embeddings\n",
    "  you can do that to. \n",
    "- Avoid modules within modules within modules to created models that are combined. \n",
    "- Not flexible. It is possible to make the first encode/decode model... But adding a preprocessor, swapping out a backbone,\n",
    "  adding additional heads or necks and sharing computations will typically not be easy. I ended up creating multiple modules that are\n",
    "  called within other modules... All head/modules pass dictionaries between modules. \n",
    "- Typically not very reusable. \n",
    "- By packing model modules, metrics and loss-functions into a brick collection, we can more easily \n",
    "inject any desired brick collection into your custom trainer without doing modifications to the trainer.\n",
    "\n",
    "Including metrics and losses with the model. \n",
    "- Model, metrics and losses are connected. If we want to add an additional head to a model - we should also add losses and metrics. \n",
    "- The typical distinction between `encode`  / `decoder` becomes to limited... Multiple decoders might share a `neck`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brick features: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Export as ONNX\n",
    "To export a brick collection as onnx we provide the `export_bricks_as_onnx`-function. \n",
    "\n",
    "Pass an example input (`named_input`) to trace a brick collection.\n",
    "Set `dynamic_batch_size=True` to support any batch size inputs and here we explicitly set `stage=Stage.EXPORT` - this is also \n",
    "the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torchbricks.brick_utils import export_bricks_as_onnx\n",
    "path_onnx = Path(\"build/readme_model.onnx\")\n",
    "export_bricks_as_onnx(path_onnx=path_onnx, \n",
    "                      brick_collection=brick_collection, \n",
    "                      named_inputs=named_inputs, \n",
    "                      dynamic_batch_size=True, \n",
    "                      stage=Stage.EXPORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Act as a nn.Module\n",
    "A brick collection acts as a 'nn.Module' meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to specify device (CPU/GPU) or precision to automatically move model parameters\n",
    "brick_collection.to(torch.float16)\n",
    "brick_collection.to(torch.float32)\n",
    "\n",
    "# Save model parameters\n",
    "path_model = Path(\"build/readme_model.pt\")\n",
    "torch.save(brick_collection.state_dict(), path_model)\n",
    "\n",
    "# Load model parameters\n",
    "brick_collection.load_state_dict(torch.load(path_model))\n",
    "\n",
    "# Access parameters\n",
    "brick_collection.named_parameters()\n",
    "\n",
    "# Using compile with pytorch >= 2.0\n",
    "torch.compile(brick_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Nested bricks and relative input/output names\n",
    "To more easily add, remove and swap out a subset of bricks in a brick collection (e.g. bricks related to specific task), we\n",
    "support passing a nested dictionary of bricks to a `BrickCollection` and using relative input and output names. \n",
    "\n",
    "Both nested dictionaries and relative input and output names are demonstrated below by adding multiple classifications \n",
    "heads (`head0` and `head1`) to a brick collection. Each classification head is created with `create_image_classification_head`. \n",
    "\n",
    "Note also that the function uses relative names such as `./logits`, `./probabilities`, `./class_prediction` and `./loss_ce`. \n",
    "Relative names will use the brick name to derive \"absolute\" names. E.g. for `head0` the relative input name `./logits` \n",
    "becomes `head0/logits` and for `head1` the relative input name `./logits`  becomes `head1/logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from torchbricks.bricks import BrickInterface\n",
    "\n",
    "\n",
    "def create_image_classification_head(num_classes: int, in_channels: int, features_name: str, targets_name: str) -> Dict[str, BrickInterface]:\n",
    "    \"\"\"Image classifier bricks: Classifier, loss and metrics \"\"\"\n",
    "    head = {\n",
    "        'classify': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=in_channels),\n",
    "                                   input_names=[features_name], \n",
    "                                   output_names=['./logits', './probabilities', './class_prediction']),\n",
    "        'accuracy': BrickMetricSingle(MulticlassAccuracy(num_classes=num_classes), \n",
    "                                      input_names=['./class_prediction', targets_name]),\n",
    "        'loss': BrickLoss(model=nn.CrossEntropyLoss(),\n",
    "                          input_names=['./logits', targets_name], \n",
    "                          output_names=['./loss_ce'])\n",
    "    }\n",
    "    return head\n",
    "\n",
    "n_features = resnet_brick.model.n_backbone_features\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), \n",
    "                                      input_names=['raw'], \n",
    "                                      output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'head0': create_image_classification_head(num_classes=3, in_channels=n_features, features_name='features', targets_name='targets'),\n",
    "    'head1': create_image_classification_head(num_classes=5, in_channels=n_features, features_name='features', targets_name='targets'),\n",
    "}\n",
    "brick_collections = BrickCollection(bricks)\n",
    "print(brick_collections)\n",
    "print(create_mermaid_dag_graph(brick_collections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mermaid visualization of above graph: \n",
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(\"<strong>BrickNotTrainable</strong><br><strong>preprocessor</strong>: Preprocessor\"):::BrickNotTrainable\n",
    "    backbone(\"<strong>BrickTrainable</strong><br><strong>backbone</strong>: BackboneResnet\"):::BrickTrainable\n",
    "    head0/classify(\"<strong>BrickTrainable</strong><br><strong>head0/classify</strong>: ImageClassifier\"):::BrickTrainable\n",
    "    head0/accuracy(\"<strong>BrickMetricSingle</strong><br><strong>head0/accuracy</strong>: ['MulticlassAccuracy']\"):::BrickMetricSingle\n",
    "    head0/loss(\"<strong>BrickLoss</strong><br><strong>head0/loss</strong>: CrossEntropyLoss\"):::BrickLoss\n",
    "    head1/classify(\"<strong>BrickTrainable</strong><br><strong>head1/classify</strong>: ImageClassifier\"):::BrickTrainable\n",
    "    head1/accuracy(\"<strong>BrickMetricSingle</strong><br><strong>head1/accuracy</strong>: ['MulticlassAccuracy']\"):::BrickMetricSingle\n",
    "    head1/loss(\"<strong>BrickLoss</strong><br><strong>head1/loss</strong>: CrossEntropyLoss\"):::BrickLoss\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw:::input --> preprocessor\n",
    "    targets:::input --> head0/accuracy\n",
    "    targets:::input --> head0/loss\n",
    "    targets:::input --> head1/accuracy\n",
    "    targets:::input --> head1/loss\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |normalized| backbone\n",
    "    backbone --> |features| head0/classify\n",
    "    backbone --> |features| head1/classify\n",
    "    subgraph head0\n",
    "        head0/classify --> |head0/class_prediction| head0/accuracy\n",
    "        head0/classify --> |head0/logits| head0/loss\n",
    "        head0/classify --> head0/probabilities:::output\n",
    "        head0/loss --> head0/loss_ce:::output\n",
    "    end\n",
    "    subgraph head1\n",
    "        head1/classify --> |head1/class_prediction| head1/accuracy\n",
    "        head1/classify --> |head1/logits| head1/loss\n",
    "        head1/classify --> head1/probabilities:::output\n",
    "        head1/loss --> head1/loss_ce:::output\n",
    "    end\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    classDef BrickMetricSingle stroke-width:0px,fill:#1450A3 \n",
    "    classDef BrickLoss stroke-width:0px,fill:#5C677D \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Bag of bricks - reusable bricks modules\n",
    "Note also in above example we use bag-of-bricks to import commonly used `nn.Module`s \n",
    "\n",
    "This includes a `Preprocessor`, `ImageClassifier` and `resnet_to_brick` to convert a torchvision resnet models to a backbone brick \n",
    "without a classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Training with pytorch-lightning trainer\n",
    "I like and love pytorch-lightning! We can avoid writing the easy-to-get-wrong training loop and validation/test scrips.\n",
    "\n",
    "Pytorch lightning creates logs, ensures training is done efficiently on any device (CPU, GPU, TPU), on multiple/distributed devices \n",
    "with reduced precision and much more.\n",
    "\n",
    "However, one issue I found myself having when wanting to extend my custom pytorch-lightning module (`LightningModule`) is that it forces an\n",
    "object oriented style with multiple levels of inheritance. This is not necessarily bad, but it makes it hard to reuse \n",
    "code across projects and generally makes the code complicated. \n",
    "\n",
    "With a brick collection you should rarely change or inherit your lightning module, instead you can inject the model, metrics and loss functions\n",
    "into a lightning module. Changes to preprocessor, backbone, necks, heads, metrics and losses are done on the outside\n",
    "and injected into the lightning module. \n",
    "\n",
    "Below is an example of how you could inject a brick collection with pytorch-lightning. \n",
    "\n",
    "We have created `LightningBrickCollection` ([available here](https://github.com/PeteHeine/torchbricks/blob/main/scripts/lightning_module.py)) \n",
    "as an example for you to use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from utils_testing.lightning_module import LightningBrickCollection\n",
    "from utils_testing.datamodule_cifar10 import CIFAR10DataModule\n",
    "\n",
    "experiment_name=\"CIFAR10\"\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "data_module = CIFAR10DataModule(data_dir='data', batch_size=5, num_workers=12, test_transforms=transform, train_transforms=transform)\n",
    "create_opimtizer_func = partial(torch.optim.SGD, lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "bricks_lightning_module = LightningBrickCollection(path_experiments=Path(\"build\") / \"experiments\",\n",
    "                                                   experiment_name=None,\n",
    "                                                   brick_collection=brick_collection,\n",
    "                                                   create_optimizers_func=create_opimtizer_func)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n",
    "# Train and test model by injecting 'bricks_lightning_module'\n",
    "# trainer.fit(bricks_lightning_module, datamodule=data_module)\n",
    "# trainer.test(bricks_lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Brick features: Pass all intermediate tensors to Brick\n",
    "By adding `'__all__'` to `input_names`, it is possible to access all tensors as a dictionary inside a brick module. \n",
    "For production code, this may not be the best option, but this feature can be valuable during an exploration phase or \n",
    "when doing some live debugging of a new model/module. \n",
    "\n",
    "We will demonstrate in code by introducing a (dummy) module `VisualizeRawAndPreprocessed`.\n",
    "\n",
    "*Note: It is just a dummy class, don't worry to much about the actual implementation.*\n",
    "\n",
    "The important thing to notice is that `input_names = ['__all__']` is used for our `visualizer`-brick to\n",
    "pass all tensors as a dictionary as an argument in the forward call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizeRawAndPreprocessed(torch.nn.Module):\n",
    "    def forward(self, named_inputs: Dict[str, Any]):\n",
    "        ## Here `named_inputs` contains all intermediate tensors\n",
    "        image_raw_and_preprocessed = torch.concatenate((named_inputs[\"raw\"], named_inputs[\"preprocessed\"]), dim=3)\n",
    "        return image_raw_and_preprocessed\n",
    "\n",
    "\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), input_names=['raw'],  output_names=['preprocessed']),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=3, n_features=10), input_names=['preprocessed'], output_names=['embedding']),\n",
    "    'visualizer': BrickNotTrainable(VisualizeRawAndPreprocessed(), input_names = ['__all__'], output_names=[\"visualization\"])\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "named_outputs = brick_collection(named_inputs={'raw': torch.rand((2, 3, 100, 200))}, stage=Stage.INFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brick features: Using Stage Inside Module\n",
    "By passing in `stage` in `input_names` it is possible to change the program flow. \n",
    "\n",
    "As demonstrated below want to alway resize the input to a specific size when the model is being exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(torch.nn.Module):\n",
    "    def forward(self, input_image: torch.Tensor, stage: Stage) -> str:\n",
    "        if stage in [Stage.EXPORT]:\n",
    "            input_image = torch.nn.functional.interpolate(input_image, size=(50,100))\n",
    "        return input_image/2\n",
    "\n",
    "\n",
    "brick_collection = BrickCollection({\n",
    "        \"preprocessor\": BrickNotTrainable(Preprocessor(), input_names=['raw', 'stage'], output_names=[\"processed\"])\n",
    "    })\n",
    "named_inputs = {'raw': torch.rand((2, 3, 100, 200))}\n",
    "named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.EXPORT)\n",
    "assert list(named_outputs[\"processed\"].shape[2:]) == [50, 100]\n",
    "\n",
    "named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.VALIDATION)\n",
    "assert list(named_outputs[\"processed\"].shape[2:]) == [100, 200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why should I explicitly set the train, val or test stage\n",
    "\n",
    "MISSING\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "## What are we missing?\n",
    "\n",
    "\n",
    "- [x] ~~Proper~~ Added a link to `LightningBrickCollection` for other people to use\n",
    "- [x] Minor: BrickCollections supports passing a dictionary with BrickCollections. But we should also convert a nested dictionary into a nested brick collections\n",
    "- [x] Minor: Currently, `input_names` and `output_names` support positional arguments, but we should also support keyword arguments.\n",
    "- [x] Minor: Make Brick an abstract class\n",
    "- [x] Convert torchvision resnet models to only a backbone brick.\n",
    "- [x] Make readme a notebook\n",
    "- [x] Automatically convert jupyter notebook to `README.md`\n",
    "- [x] Remove README.md header\n",
    "- [x] Make an export to onnx function \n",
    "- [x] Make it optional if gradients can be passed through NonTrainableBrick without weights being optimized\n",
    "- [x] Refactor Metrics: Create BrickMetricCollection and BrickSingleMetric and create flag to return metrics.\n",
    "- [x] Make brick base class with input_names, output_names and alive_stages - inherit this from other bricks. \n",
    "  - Pros: We might include other non-torch modules later. \n",
    "  - Do not necessarily pass a stage-object. Consider also passing it as a string so it can be handled correctly with scripting. \n",
    "- [x] Update README.md to match the new bricks. \n",
    "  - [x] Start with basic bricks example. \n",
    "  - [x] Use loss-function to show that stage decided on what is being executed. \n",
    "  - [x] Introduce metrics by it-self in another example\n",
    "- [x] Ensure that all examples in the `README.md` are working with easy to use modules. \n",
    "- [x] Add typeguard\n",
    "- [x] Allow a brick to receive all named_inputs and add a test for it.\n",
    "- [x] Fix the release process. It should be as simple as running `make release`.\n",
    "- [x] Add onnx export example to the README.md\n",
    "- [x] Pretty print bricks\n",
    "- [x] Relative input/output names\n",
    "- [x] Test to verify that environment matches conda lock. The make command 'update-lock-file' should store a copy of 'environment.yml'\n",
    "      We will the have a test checking if the copy and the current version of `environment.yml` is the same.\n",
    "- [x] Add code coverage and tests passed badges to readme again\n",
    "- [x] Create brick-collection visualization tool (\"mermaid?\")\n",
    "- [x] Make DAG like functionality to check if a inputs and outputs works for all model stages.\n",
    "- [ ] Make common Visualizations with pillow - not opencv to not blow up the required dependencies. ImageClassification, Segmentation, ObjectDetection\n",
    "  - [ ] Maybe visualizations should be done in OpenCV it is faster. \n",
    "- [ ] Use pymy, pyright or pyre to do static code checks. \n",
    "- [ ] Decide: Add stage as an internal state and not in the forward pass:\n",
    "  - Minor Pros: Tracing (to get onnx model) requires only torch.Tensors only as input - we avoid making an adapter class. \n",
    "  - Minor Cons: State gets hidden away - implicit instead of explicit.\n",
    "  - Minor Pros: Similar to eval/training \n",
    "- [ ] Collection of helper modules. Preprocessors, Backbones, Necks/Upsamplers, ImageClassification, SemanticSegmentation, ObjectDetection\n",
    "  - [ ] All the modules in the README should be easy to import as actually modules.\n",
    "  - [ ] Make common brick collections: BricksImageClassification, BricksSegmentation, BricksPointDetection, BricksObjectDetection\n",
    "- [ ] Support preparing data in the dataloader?\n",
    "- [ ] Support torch.jit.scripting? \n",
    "\n",
    "## How does it really work?\n",
    "????\n",
    "\n",
    "\n",
    "\n",
    "## Development\n",
    "\n",
    "Read the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n",
    "\n",
    "### Install\n",
    "\n",
    "    conda create --name torchbricks --file conda-linux-64.lock\n",
    "    conda activate torchbricks\n",
    "    poetry install\n",
    "\n",
    "### Activating the environment\n",
    "\n",
    "    conda activate torchbricks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
