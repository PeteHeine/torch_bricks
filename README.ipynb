{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchBricks\n",
    "\n",
    "[![codecov](https://codecov.io/gh/PeteHeine/torchbricks/branch/main/graph/badge.svg?token=torchbricks_token_here)](https://codecov.io/gh/PeteHeine/torchbricks)\n",
    "[![CI](https://github.com/PeteHeine/torchbricks/actions/workflows/main.yml/badge.svg)](https://github.com/PeteHeine/torchbricks/actions/workflows/main.yml)\n",
    "\n",
    "TorchBricks builds pytorch models using small reuseable and decoupled parts - we call them bricks.\n",
    "\n",
    "The concept is simple and flexible and allows you to more easily combine and swap out parts of the model (preprocessor, backbone, neck, head or post-processor), change the task or extend it with multiple tasks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Install it with pip\n",
    "\n",
    "```bash\n",
    "pip install torchbricks\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basic use-case: Image classification\n",
    "Let us see it in action:\n",
    "\n",
    "First we specify regular pytorch modules: A preprocessor, a model and a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class PreprocessorDummy(nn.Module):\n",
    "    def forward(self, raw_input: torch.Tensor) -> torch.Tensor:\n",
    "        return raw_input/2\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_features, 1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(tensor)\n",
    "\n",
    "class ClassifierDummy(nn.Module):\n",
    "    def __init__(self, num_classes: int, in_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(torch.flatten(self.avgpool(tensor)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use torchbricks to define how the modules are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchbricks.bricks import BrickCollection, BrickNotTrainable, BrickTrainable\n",
    "from torchbricks.bricks import Phase\n",
    "\n",
    "# Defining model from bricks\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), input_names=['raw'], output_names=['processed']),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=3, n_features=10), input_names=['processed'], output_names=['embedding']),\n",
    "    'image_classifier': BrickTrainable(ClassifierDummy(num_classes=3, in_features=10), input_names=['embedding'], output_names=['logits'])\n",
    "}\n",
    "\n",
    "# Executing model\n",
    "brick_collection = BrickCollection(bricks)\n",
    "batch_image_example = torch.rand((1, 3, 100, 200))\n",
    "outputs = brick_collection(named_inputs={'raw': batch_image_example}, phase=Phase.TRAIN)\n",
    "print(outputs.keys())\n",
    "\n",
    "brick_collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All modules are added as entries in a regular dictionary, and for each module we 1) specify a name \n",
    "2) if it is trainable or not (`BrickTrainable`/`BrickNotTrainable`) and 3) input and output names. \n",
    "   \n",
    "Finally, bricks are collected in a `BrickCollection`. A `BrickCollection` is a \n",
    "regular `nn.Module` with a `forward`-function, `to` to move to a specific device/precision, \n",
    "you can save/load a model, management of parameters, export model as either onnx/TorchScript etc. \n",
    "\n",
    "Furthermore a brick collection acts as a simple DAG, it accepts a dictionary (`named_inputs`), \n",
    "executes each bricks and ensures that the outputs are passed to the inputs of other bricks with matching names. \n",
    "Structuring the model as a DAG, makes it easy to add/remove outputs for a given module, add new modules to a given model\n",
    "and build completely new models from reusable parts. \n",
    "\n",
    "Note also that we set `phase=Phase.TRAIN` to explicitly specify if we are doing training, validation, test or inference.\n",
    "Specifying a phase is important, if we want a module to act in a specific way during specific phases.\n",
    "We will get back to this later. \n",
    "\n",
    "Not shown here, a `BrickCollection` also supports a nested dictionary of bricks. A nested brick collections acts the same, \n",
    "but it becomes easier to add and remove sub-collections bricks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of bricks - reusable bricks modules\n",
    "We provide a bag-of-bricks with commonly used `nn.Module`s \n",
    "\n",
    "Below we create a brick collection with a real world example including a `Preprocessor`, an adaptor function to convert\n",
    "torchvision resnet models into a backbone brick (with no classifier) and an `ImageClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "from torchbricks.bag_of_bricks import ImageClassifier, resnet_to_brick, Preprocessor\n",
    "\n",
    "num_classes = 10\n",
    "resnet_brick = resnet_to_brick(resnet=resnet18(weights=False, num_classes=num_classes),  input_name='normalized', output_name='features')\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), input_names=['raw'], output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'image_classifier': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=resnet_brick.model.n_backbone_features),\n",
    "                                     input_names=['features'], output_names=['logits', 'probabilities', 'class_prediction']),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case: Bricks `on_step`-function for training and evaluation\n",
    "In above examples, we have showed how to compose the model with trainable and non-trainable bricks, and how a dictionary of tensors \n",
    "is passed to the forward function... But TorchBricks goes beyond that.\n",
    "\n",
    "An important feature of a brick collection is that is the `on_step`-function to also calculate metrics and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchbricks.bricks import BrickLoss, BrickMetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), input_names=['raw'], output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'image_classifier': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=resnet_brick.model.n_backbone_features),\n",
    "                                     input_names=['features'], output_names=['logits', 'probabilities', 'class_prediction']),\n",
    "    'accuracy': BrickMetricCollection(MulticlassAccuracy(num_classes=num_classes), input_names=['class_prediction', 'targets'], \n",
    "                                      metric_name=\"Accuracy\"),\n",
    "    'loss': BrickLoss(model=nn.CrossEntropyLoss(), input_names=['logits', 'targets'], output_names=['loss_ce'])\n",
    "}\n",
    "\n",
    "# We can still run the forward-pass as before - Note: The forward call does not require 'targets'\n",
    "brick_collection = BrickCollection(bricks)\n",
    "batch_image_example = torch.rand((1, 3, 100, 200))\n",
    "outputs = brick_collection(named_inputs={\"raw\": batch_image_example}, phase=Phase.TRAIN)\n",
    "\n",
    "# Example of running `on_step`. Note: `on_step` requires `targets` to calculate metrics and loss.\n",
    "named_inputs = {\"raw\": batch_image_example, \"targets\": torch.ones((1), dtype=torch.int64)}\n",
    "named_outputs, losses = brick_collection.on_step(phase=Phase.TRAIN, named_inputs=named_inputs, batch_idx=0)\n",
    "named_outputs, losses = brick_collection.on_step(phase=Phase.TRAIN, named_inputs=named_inputs, batch_idx=1)\n",
    "named_outputs, losses = brick_collection.on_step(phase=Phase.TRAIN, named_inputs=named_inputs, batch_idx=2)\n",
    "metrics = brick_collection.summarize(phase=Phase.TRAIN, reset=True)\n",
    "print(f\"{metrics=}, {losses=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example we extend our brick collection with a `BrickTorchMetric` brick for handling metrics and a `BrickLoss` to handle our \n",
    "loss-function. \n",
    "\n",
    "For metrics, we rely on the [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/) library and passes either a single \n",
    "metric (`torchmetrics.Metric`) or collection of metrics (`torchmetrics.MetricCollection`) to `BrickTorchMetric`. \n",
    "\n",
    "Note also that we continue to use input names and output names to easily define how modules are connected. \n",
    "\n",
    "On each `on_step`, we calculate model outputs, losses and metrics for each batch. Metrics are aggregated internally in `BrickTorchMetric` \n",
    "and only returned with the `summarize`-call. We set `reset=True` to reset metric aggregation. \n",
    "\n",
    "Note also that our metric (`Accuracy`) has been added a prefix, so it becomes `train/Accuracy` and demonstrates the "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case: Training with a collections of bricks\n",
    "By packing model modules, metrics and loss-functions into a brick collection and providing a `on_step`-function, we can more easily \n",
    "inject any desired brick collection into your custom trainer without doing modifications to trainer.\n",
    "\n",
    "### Use-case: Training with pytorch-lightning trainer\n",
    "I like and love pytorch-lightning! We can avoid writing the easy-to-get-wrong training loop, write validation/test scrips.\n",
    "\n",
    "Pytorch lightning will create logs, ensures training is done efficiently on any device (CPU, GPU, TPU), on multiple/distributed devices \n",
    "with reduced precision and much more.\n",
    "\n",
    "However, one issue I found myself having when wanting to extend my custom pytorch-lightning module (`LightningModule`) is that it forces an\n",
    "object oriented style with multiple levels of inheritance. This is not necessarily bad, but it makes it hard to reuse \n",
    "code across projects and generally made the code complicated. \n",
    "\n",
    "With a brick collection you should rarely change or inherit your lightning module, instead you inject the model, metrics and loss functions\n",
    "into a lightning module. Changes to preprocessor, backbone, necks, heads, metrics and losses are done on the outside\n",
    "and injected into the lightning module. \n",
    "\n",
    "Below is an example of how you could inject a brick collection into with pytorch-lightning. \n",
    "We have created `LightningBrickCollection` ([available here](https://github.com/PeteHeine/torchbricks/blob/main/scripts/lightning_module.py)) \n",
    "as an example for you to use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from utils_testing.lightning_module import LightningBrickCollection\n",
    "from utils_testing.datamodule_cifar10 import CIFAR10DataModule\n",
    "\n",
    "experiment_name=\"CIFAR10\"\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "data_module = CIFAR10DataModule(data_dir='data', batch_size=5, num_workers=12, test_transforms=transform, train_transforms=transform)\n",
    "create_opimtizer_func = partial(torch.optim.SGD, lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "bricks_lightning_module = LightningBrickCollection(path_experiments=Path(\"build\") / \"experiments\",\n",
    "                                                   experiment_name=None,\n",
    "                                                   brick_collection=brick_collection,\n",
    "                                                   create_optimizers_func=create_opimtizer_func)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=1, limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n",
    "# To train and test model\n",
    "trainer.fit(bricks_lightning_module, datamodule=data_module)\n",
    "trainer.test(bricks_lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic use-case: Semantic Segmentation\n",
    "After running experiments, we now realize that we also wanna do semantic segmentation.\n",
    "This is how it would look like:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By wrapping both core model computations, metrics and loss functions into a single brick collection, we can more easily swap between\n",
    "running model experiments in notebooks, trainings\n",
    "\n",
    "We provide a `forward` function to easily run model inference without targets and an `on_step` function\n",
    "to easily get metrics and losses in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_implementation = True\n",
    "if missing_implementation:\n",
    "    print(\"MISSING\")\n",
    "else:\n",
    "    # We can optionally keep/remove image_classification from before\n",
    "    bricks.pop(\"image_classifier\")\n",
    "\n",
    "    # Add upscaling and semantic segmentation nn.Modules\n",
    "    bricks[\"upscaling\"] = BrickTrainable(Upscaling(), input_names=[\"embedding\"], output_names=[\"embedding_upscaled\"])\n",
    "    bricks[\"semantic_segmentation\"] = BrickTrainable(SegmentationClassifier(), input_names=[\"embedding_upscaled\"], output_names=[\"ss_logits\"])\n",
    "\n",
    "    # Executing model\n",
    "    brick_collection = BrickCollection(bricks)\n",
    "    batch_image_example = torch.rand((1, 3, 100, 200))\n",
    "    outputs = brick_collection(named_inputs={\"raw\": batch_image_example}, phase=Phase.TRAIN)\n",
    "\n",
    "    print(outputs.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested brick collections\n",
    "It can handle nested brick collections and nested dictionary of bricks.\n",
    "\n",
    "MISSING\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TorchMetric.MetricCollection\n",
    "\n",
    "MISSING\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why should I explicitly set the train, val or test phase\n",
    "\n",
    "MISSING\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "## What are we missing?\n",
    "\n",
    "\n",
    "- [x] ~~Proper~~ Added a link to `LightningBrickCollection` for other people to use\n",
    "- [x] Minor: BrickCollections supports passing a dictionary with BrickCollections. But we should also convert a nested dictionary into a nested brick collections\n",
    "- [x] Minor: Currently, `input_names` and `output_names` support positional arguments, but we should also support keyword arguments.\n",
    "- [x] Minor: Make Brick an abstract class\n",
    "- [x] Convert torchvision resnet models to only a backbone brick.\n",
    "- [x] Make readme a notebook\n",
    "- [x] Automatically convert jupyter notebook to `README.md`\n",
    "- [x] Remove README.md header\n",
    "- [x] Make an export to onnx function \n",
    "- [x] Make it optional if gradients can be passed through NonTrainableBrick without weights being optimized\n",
    "- [ ] Refactor MetricCollection to have flag to return metrics.\n",
    "- [ ] Update README.md to match the new bricks. \n",
    "  - [ ] Start with basic bricks example. \n",
    "  - [ ] Use loss-function to show that Phase decided on what is being executed. \n",
    "  - [ ] Introduce metrics by it-self in another example\n",
    "- [ ] Add onnx export example to the README.md\n",
    "- [ ] Make brick base class with input_names, output_names and run_on - inherit this from other bricks. \n",
    "  - [ ] Pros: We might include other non-torch modules later. \n",
    "  - [ ] Do not necessarily pass a Phase-object. Consider also passing it as a string so it can be handled correctly with scripting. \n",
    "- [ ] Ensure that all examples in the `README.md` are working with easy to use modules. \n",
    "- [ ] Make DAG like functionality to check if a inputs and outputs works for all model phases.\n",
    "- [ ] Use pymy, pyright or pyre to do static code checks. \n",
    "- [ ] Decide: Add phase as an internal state and not in the forward pass:\n",
    "  - Minor Pros: Tracing (to get onnx model) requires only torch.Tensors only as input - we avoid making an adapter class. \n",
    "  - Minor Cons: State gets hidden away - implicit instead of explicit.\n",
    "  - Minor Pros: Similar to eval/training \n",
    "- [ ] Collection of helper modules. Preprocessors, Backbones, Necks/Upsamplers, ImageClassification, SemanticSegmentation, ObjectDetection\n",
    "  - [ ] All the modules in the README should be easy to import as actually modules.\n",
    "  - [ ] Make common brick collections: BricksImageClassification, BricksSegmentation, BricksPointDetection, BricksObjectDetection\n",
    "- [ ] Support preparing data in the dataloader?\n",
    "- [ ] Make common Visualizations with pillow - not opencv to not blow up the required dependencies. ImageClassification, Segmentation, ObjectDetection\n",
    "\n",
    "## How does it really work?\n",
    "????\n",
    "\n",
    "\n",
    "\n",
    "## Development\n",
    "\n",
    "Read the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n",
    "\n",
    "### Install\n",
    "\n",
    "    conda create --name torchbricks --file conda-linux-64.lock\n",
    "    conda activate torchbricks\n",
    "    poetry install\n",
    "\n",
    "### Activating the environment\n",
    "\n",
    "    conda activate torchbricks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
