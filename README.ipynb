{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchBricks\n",
    "[![codecov](https://codecov.io/gh/pete-machine/torchbricks/branch/main/graph/badge.svg?token=torchbricks_token_here)](https://codecov.io/gh/pete-machine/torchbricks)\n",
    "[![CI](https://github.com/pete-machine/torchbricks/actions/workflows/main.yml/badge.svg)](https://github.com/pete-machine/torchbricks/actions/workflows/main.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchBricks builds pytorch models using small reuseable and decoupled parts - we call them bricks. \n",
    "\n",
    "The concept is simple and flexible and allows you to more easily combine, add or swap out parts of the model \n",
    "(preprocessor, backbone, neck, head or post-processor), change the task or extend it with multiple tasks.\n",
    "\n",
    "TorchBricks is a compact recipe on both *how* model parts are connected and *when* parts are executed \n",
    "during model stages such as training, validation, testing, inference and export."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Install it with pip\n",
    "\n",
    "```bash\n",
    "pip install torchbricks\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricks by example\n",
    "\n",
    "To demonstrate the the concepts of TorchBricks, we will first specify some dummy parts used in a regular image recognition model: \n",
    "A preprocessor, a backbone and a head (in this case a classifier).\n",
    "*Note: Don't worry about the actually implementation of these modules - they are just dummy examples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PreprocessorDummy(nn.Module):\n",
    "    def forward(self, raw_input: torch.Tensor) -> torch.Tensor:\n",
    "        return raw_input/2\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(tensor)\n",
    "\n",
    "class ClassifierDummy(nn.Module):\n",
    "    def __init__(self, num_classes: int, in_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.fc(torch.flatten(self.avgpool(tensor), start_dim = 1))\n",
    "        return logits, self.softmax(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concept 1: Bricks are connected\n",
    "An important concept of TorchBricks is that it defines how modules are connected by specifying input and output names of\n",
    "each module similar to a DAG. \n",
    "\n",
    "In below code snippet, we demonstrate how this would look for our dummy model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchbricks.bricks import BrickCollection, BrickLoss, BrickNotTrainable, BrickTrainable, Stage\n",
    "from torchbricks.graph_plotter import create_mermaid_dag_graph\n",
    "\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), \n",
    "                                      input_names=['raw_images'], \n",
    "                                      output_names=['processed']),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=3, n_features=10), \n",
    "                               input_names=['processed'], \n",
    "                               output_names=['embedding']),\n",
    "    'head': BrickTrainable(ClassifierDummy(num_classes=3, in_features=10), \n",
    "                           input_names=['embedding'], \n",
    "                           output_names=['logits', \"softmaxed\"]),\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "# print(create_mermaid_dag_graph(brick_collection))\n",
    "print(brick_collection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each module is placed in a dictionary with a unique name and wrapped inside a brick with input and output names. \n",
    "Input and output names specifies how outputs of one module is passed to inputs of the next module. \n",
    "\n",
    "In above example, we use `BrickNotTrainable` to wrap modules that are shouldn't be trained (weights are fixed) and \n",
    "`BrickTrainable` to wrap modules that are trainable (weights are updated on each training iteration). \n",
    "\n",
    "Finally, the dictionary of bricks is passed to a `BrickCollection`. \n",
    "\n",
    "Below we visualize how the brick collection connects bricks together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(<strong>'preprocessor': PreprocessorDummy</strong><br><i>BrickNotTrainable</i>):::BrickNotTrainable\n",
    "    backbone(<strong>'backbone': TinyModel</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    head(<strong>'head': ClassifierDummy</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw_images:::input --> preprocessor\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |processed| backbone\n",
    "    backbone --> |embedding| head\n",
    "    head --> logits:::output\n",
    "    head --> softmaxed:::output\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```\n",
    "*Graph is visualized using [mermaid](https://github.com/mermaid-js/mermaid) syntax.*\n",
    "*We provide the `create_mermaid_dag_graph`-function to create a brick collection visualization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BrickCollection` is used for executing above graph by passing a dictionary with named input data (`named_inputs`). \n",
    "\n",
    "For above brick collection, we only expect one named input called `raw_images`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "batched_images = torch.rand((batch_size, 3, 100, 200))\n",
    "named_inputs = {'raw_images': batched_images}\n",
    "named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.INFERENCE)\n",
    "print(\"Brick outputs:\", named_outputs.keys())\n",
    "# Brick outputs: dict_keys(['raw_images', 'stage', 'processed', 'embedding', 'logits', 'softmaxed'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brick collection accepts a dictionary and returns a dictionary with all intermediated and resulting tensors. \n",
    "\n",
    "Running our models as a brick collection has the following advantages:\n",
    "\n",
    "- A brick collection act as a regular `nn.Module` with all the familiar features: a `forward`-function, a `to`-function to move \n",
    "  to a specific device/precision, you can save/load a model, management of parameters, onnx exportable etc. \n",
    "- A brick collection is also a simple DAG, it accepts a dictionary with \"named data\" (we call this `named_inputs`), \n",
    "executes each bricks and ensures that the outputs are passed to the inputs of other bricks with matching names. \n",
    "Structuring the model as a DAG, makes it easy to add/remove outputs for a given module during development, add new modules to the\n",
    "collection and build completely new models from reusable parts. \n",
    "- A brick collection is actually a dictionary (`nn.DictModule`). Allowing you to access, pop and update the \n",
    "  collection easily as a regular dictionary. It can also handle nested dictionary, allowing groups of bricks to be added/removed easily. \n",
    "\n",
    "Note also that we set `stage=Stage.INFERENCE` to explicitly specify if we are doing training, validation, test or inference.\n",
    "Specifying a stage is important, if we want a module to act in a specific way during a specific stages.\n",
    "\n",
    "Leading us to the next section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept 2: Bricks can be dead (or alive)\n",
    "The second concept is to specify when bricks are alive - meaning we can specify at which stages (train, test, validation, inference \n",
    "and export) a brick is active.\n",
    "\n",
    "In above example this is not very interesting - because a model will mostly have preprocessor, backbone and head active \n",
    "during all stages.\n",
    "\n",
    "So we will demonstrate by adding a loss brick (`BrickLoss`) and specifying `alive_stages` for each brick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(PreprocessorDummy(), \n",
    "                                      input_names=['raw_images'], \n",
    "                                      output_names=['processed'], \n",
    "                                      alive_stages=\"all\"),\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=num_classes, n_features=10), \n",
    "                               input_names=['processed'], \n",
    "                               output_names=['embedding'], \n",
    "                               alive_stages=\"all\"),\n",
    "    'head': BrickTrainable(ClassifierDummy(num_classes=num_classes, in_features=10), \n",
    "                           input_names=['embedding'], \n",
    "                           output_names=['logits', 'softmaxed'], \n",
    "                           alive_stages=\"all\"),\n",
    "    'loss': BrickLoss(model=nn.CrossEntropyLoss(), \n",
    "                      input_names=['logits', 'targets'], \n",
    "                      output_names=['loss_ce'], \n",
    "                      alive_stages=[Stage.TRAIN, Stage.VALIDATION, Stage.TEST])\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "\n",
    "print(brick_collection)\n",
    "# BrickCollection(\n",
    "#   (preprocessor): BrickNotTrainable(PreprocessorDummy, input_names=['raw_images'], output_names=['processed'], alive_stages=['TRAIN', 'VALIDATION', 'TEST', 'INFERENCE', 'EXPORT'])\n",
    "#   (backbone): BrickTrainable(TinyModel, input_names=['processed'], output_names=['embedding'], alive_stages=['TRAIN', 'VALIDATION', 'TEST', 'INFERENCE', 'EXPORT'])\n",
    "#   (head): BrickTrainable(ClassifierDummy, input_names=['embedding'], output_names=['logits', 'softmaxed'], alive_stages=['TRAIN', 'VALIDATION', 'TEST', 'INFERENCE', 'EXPORT'])\n",
    "#   (loss): BrickLoss(CrossEntropyLoss, input_names=['logits', 'targets'], output_names=['loss_ce'], alive_stages=['TRAIN', 'VALIDATION', 'TEST'])\n",
    "# )\n",
    "print(create_mermaid_dag_graph(brick_collection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set `preprocessor`, `backbone` and `head` to be alive on all stages `alive_stages=\"all\"` - this is the default behavior and\n",
    "similar to before. \n",
    " \n",
    "For `loss` we set `alive_stages=[Stage.TRAIN, Stage.VALIDATION, Stage.TEST]` to only calculate loss during train, validation and test\n",
    "stages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph during train, test and validation:**\n",
    "\n",
    "During `Stage.TRAIN`, `Stage.VALIDATION` and `Stage.TEST`, the loss module is alive and note now that \n",
    "both `raw_images` and `targets` are required as inputs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(<strong>'preprocessor': PreprocessorDummy</strong><br><i>BrickNotTrainable</i>):::BrickNotTrainable\n",
    "    backbone(<strong>'backbone': TinyModel</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    head(<strong>'head': ClassifierDummy</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    loss(<strong>'loss': CrossEntropyLoss</strong><br><i>BrickLoss</i>):::BrickLoss\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw_images:::input --> preprocessor\n",
    "    targets:::input --> loss\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |processed| backbone\n",
    "    backbone --> |embedding| head\n",
    "    head --> |logits| loss\n",
    "    head --> softmaxed:::output\n",
    "    loss --> loss_ce:::output\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    classDef BrickLoss stroke-width:0px,fill:#5C677D \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Graph during inference and export:**\n",
    "\n",
    "During `Stage.INFERENCE` and `Stage.EXPORT`, the graph will look as before, the loss modules is dead and note that `raw_images` is \n",
    "the only required input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(<strong>'preprocessor': PreprocessorDummy</strong><br><i>BrickNotTrainable</i>):::BrickNotTrainable\n",
    "    backbone(<strong>'backbone': TinyModel</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    head(<strong>'head': ClassifierDummy</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw_images:::input --> preprocessor\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |processed| backbone\n",
    "    backbone --> |embedding| head\n",
    "    head --> logits:::output\n",
    "    head --> softmaxed:::output\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated in above example, we can easily change the required inputs by change the model stage.\n",
    "That allows us to support two basic use cases:\n",
    "\n",
    "1) When labels/targets are available, we have the option of getting model prediction along with loss and metrics.\n",
    "\n",
    "2) When labels/targets are **not** available, we do only model predictions used for model inference/export.\n",
    "\n",
    "The mechanism of activating different parts of the model and making loss, metrics and visualizations part of the model recipe, \n",
    "allows us to more easily investigate/debug/visualize model parts in a notebook or scratch scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brick features: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick feature: TorchMetrics\n",
    "**We are not creating a training framework**, but to easily use the brick collection in your favorite training framework or custom \n",
    "training/validation/test loop, we need the option of **calculating model metrics** \n",
    "\n",
    "To easily inject both model, losses and metrics, we also need to easily support metrics and calculate metrics across a dataset. \n",
    "We will extend our example from before by adding metric bricks. \n",
    "\n",
    "To calculate metrics across a dataset, we heavily rely on concepts and functions used in the \n",
    "[TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/) library.\n",
    "\n",
    "The used of TorchMetrics in a brick collection is demonstrated in below code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchbricks.bag_of_bricks import ImageClassifier, Preprocessor, resnet_to_brick\n",
    "from torchbricks.bricks import BrickMetricSingle\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "num_classes = 10\n",
    "resnet = torchvision.models.resnet18(weights=None, num_classes=num_classes)\n",
    "resnet_brick = resnet_to_brick(resnet=resnet,  input_name='normalized', output_name='features')\n",
    "n_features = resnet_brick.model.n_backbone_features\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), \n",
    "                                      input_names=['raw'], \n",
    "                                      output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'head': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=n_features),\n",
    "                           input_names=['features'], \n",
    "                           output_names=['logits', 'probabilities', 'class_prediction']),\n",
    "    'accuracy': BrickMetricSingle(MulticlassAccuracy(num_classes=num_classes), \n",
    "                                  input_names=['class_prediction', 'targets']),\n",
    "    'loss': BrickLoss(model=nn.CrossEntropyLoss(), \n",
    "                      input_names=['logits', 'targets'], \n",
    "                      output_names=['loss_ce'])\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the brick collection above to simulate how a user can iterate over a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate dataloader\n",
    "named_input_simulated = {\"raw\": batched_images, \"targets\": torch.ones((batch_size), dtype=torch.int64)}\n",
    "dataloader_simulated = [named_input_simulated for _ in range(5)]\n",
    "\n",
    "# Loop over the dataset\n",
    "for named_inputs in dataloader_simulated: # Simulates iterating over the dataset\n",
    "    named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.TRAIN)\n",
    "    named_outputs_losses_only = brick_collection.extract_losses(named_outputs=named_outputs)\n",
    "\n",
    "metrics = brick_collection.summarize(stage=Stage.TRAIN, reset=True)\n",
    "print(f\"{named_outputs.keys()=}\")\n",
    "# named_outputs.keys()=dict_keys(['raw', 'targets', 'stage', 'normalized', 'features', 'logits', 'probabilities', 'class_prediction', 'loss_ce'])\n",
    "print(f\"{metrics=}\")\n",
    "# metrics={'MulticlassAccuracy': tensor(0.)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration in our (simulated) dataset, we calculate model outputs, losses and metrics for each batch. \n",
    "\n",
    "Losses are calculated and returned in `named_outputs` together with other model outputs. \n",
    "We provide `extract_losses` as simple function to filter `named_outputs` and only return losses in a new dictionary. \n",
    "\n",
    "Unlike other bricks, `BrickMetrics` will not (by default) output metrics for each batch. \n",
    "Instead metrics are stored internally in `BrickMetricSingle` and only aggregated and return when\n",
    "the `summarize` function is called. In above example, metric is aggregated over 5 batches as summaries to a single value. \n",
    "\n",
    "It is important to note that we set `reset=True` to reset the internal aggregation of metrics.  \n",
    "\n",
    "**Additional notes on metrics**\n",
    "\n",
    "You have the option of either using a single metric (`torchmetrics.Metric`) with `BrickMetricSingle` or a collection of \n",
    "metrics (`torchmetrics.MetricCollection`) with `BrickMetrics`.\n",
    "\n",
    "For multiple metrics, we advice to use `BrickMetrics` with a `torchmetrics.MetricCollection` \n",
    "[doc](https://torchmetrics.readthedocs.io/en/stable/pages/overview.html#metriccollection). \n",
    "It has some intelligent mechanisms for efficiently sharing calculation for multiple metrics.\n",
    "\n",
    "Note also that metrics are not passed to other bricks or returned as output of the brick collection - they are only stored internally. \n",
    "To also pass metrics to other bricks, you can set `return_metrics=True` for `BrickMetrics` and `BrickMetricSingle`. \n",
    "But be aware, this will add computational cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Act as a nn.Module\n",
    "A brick collection acts as a 'nn.Module' meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Move to specify device (CPU/GPU) or precision to automatically move model parameters\n",
    "brick_collection.to(torch.float16)\n",
    "brick_collection.to(torch.float32)\n",
    "\n",
    "# Save model parameters\n",
    "path_model = Path(\"build/readme_model.pt\")\n",
    "torch.save(brick_collection.state_dict(), path_model)\n",
    "\n",
    "# Load model parameters\n",
    "brick_collection.load_state_dict(torch.load(path_model))\n",
    "\n",
    "# Iterate all parameters\n",
    "for name, params in brick_collection.named_parameters():\n",
    "    pass\n",
    "\n",
    "# Iterate all layers\n",
    "for name, module in brick_collection.named_modules():\n",
    "    pass\n",
    "\n",
    "# Using compile with pytorch >= 2.0\n",
    "torch.compile(brick_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Nested bricks and relative input/output names\n",
    "To more easily add, remove and swap out a subset of bricks in a brick collection (e.g. bricks related to specific task), we\n",
    "support passing a nested dictionary of bricks to a `BrickCollection` and using relative input and output names. \n",
    "\n",
    "First we create a function (`create_image_classification_head`) that returns a dictionary with image classification specific \n",
    "bricks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from torchbricks.bricks import BrickInterface\n",
    "\n",
    "\n",
    "def create_image_classification_head(num_classes: int, in_channels: int, features_name: str, targets_name: str) -> Dict[str, BrickInterface]:\n",
    "    \"\"\"Image classifier bricks: Classifier, loss and metrics \"\"\"\n",
    "    head = {\n",
    "        'classify': BrickTrainable(ImageClassifier(num_classes=num_classes, n_features=in_channels),\n",
    "                                   input_names=[features_name], \n",
    "                                   output_names=['./logits', './probabilities', './class_prediction']),\n",
    "        'accuracy': BrickMetricSingle(MulticlassAccuracy(num_classes=num_classes), \n",
    "                                      input_names=['./class_prediction', targets_name]),\n",
    "        'loss': BrickLoss(model=nn.CrossEntropyLoss(),\n",
    "                          input_names=['./logits', targets_name], \n",
    "                          output_names=['./loss_ce'])\n",
    "    }\n",
    "    return head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the full model containing a `preprocessor`, `backbone` and two independent heads called `head0` and `head1`.\n",
    "Each head is a dictionary of bricks, making our brick collection a nested dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_features = resnet_brick.model.n_backbone_features\n",
    "bricks = {\n",
    "    'preprocessor': BrickNotTrainable(Preprocessor(), \n",
    "                                      input_names=['raw'], \n",
    "                                      output_names=['normalized']),\n",
    "    'backbone': resnet_brick,\n",
    "    'head0': create_image_classification_head(num_classes=3, in_channels=n_features, features_name='features', targets_name='targets0'),\n",
    "    'head1': create_image_classification_head(num_classes=5, in_channels=n_features, features_name='features', targets_name='targets1'),\n",
    "}\n",
    "brick_collections = BrickCollection(bricks)\n",
    "print(brick_collections)\n",
    "print(create_mermaid_dag_graph(brick_collections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also demonstrated in above example is the use of relative input and output names. \n",
    "Looking at our `create_image_classification_head` function again, you will notice that we actually use of relative input and output names \n",
    "(`./logits`, `./probabilities`, `./class_prediction` and `./loss_ce`). \n",
    "\n",
    "Relative names will use the brick name to derive \"absolute\" names. E.g. for `head0` the relative \n",
    "input name `./logits` becomes `head0/logits` and for `head1` the relative input name `./logits`  becomes `head1/logits`.\n",
    "\n",
    "We visualize above graph: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Brick definitions\n",
    "    preprocessor(<strong>'preprocessor': Preprocessor</strong><br><i>BrickNotTrainable</i>):::BrickNotTrainable\n",
    "    backbone(<strong>'backbone': BackboneResnet</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    head0/classify(<strong>'head0/classify': ImageClassifier</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    head0/accuracy(<strong>'head0/accuracy': 'MulticlassAccuracy'</strong><br><i>BrickMetricSingle</i>):::BrickMetricSingle\n",
    "    head0/loss(<strong>'head0/loss': CrossEntropyLoss</strong><br><i>BrickLoss</i>):::BrickLoss\n",
    "    head1/classify(<strong>'head1/classify': ImageClassifier</strong><br><i>BrickTrainable</i>):::BrickTrainable\n",
    "    head1/accuracy(<strong>'head1/accuracy': 'MulticlassAccuracy'</strong><br><i>BrickMetricSingle</i>):::BrickMetricSingle\n",
    "    head1/loss(<strong>'head1/loss': CrossEntropyLoss</strong><br><i>BrickLoss</i>):::BrickLoss\n",
    "    \n",
    "    %% Draw input and outputs\n",
    "    raw:::input --> preprocessor\n",
    "    targets0:::input --> head0/accuracy\n",
    "    targets0:::input --> head0/loss\n",
    "    targets1:::input --> head1/accuracy\n",
    "    targets1:::input --> head1/loss\n",
    "    \n",
    "    %% Draw nodes and edges\n",
    "    preprocessor --> |normalized| backbone\n",
    "    backbone --> |features| head0/classify\n",
    "    backbone --> |features| head1/classify\n",
    "    subgraph head0\n",
    "        head0/classify --> |head0/class_prediction| head0/accuracy\n",
    "        head0/classify --> |head0/logits| head0/loss\n",
    "        head0/classify --> head0/probabilities:::output\n",
    "        head0/loss --> head0/loss_ce:::output\n",
    "    end\n",
    "    subgraph head1\n",
    "        head1/classify --> |head1/class_prediction| head1/accuracy\n",
    "        head1/classify --> |head1/logits| head1/loss\n",
    "        head1/classify --> head1/probabilities:::output\n",
    "        head1/loss --> head1/loss_ce:::output\n",
    "    end\n",
    "    \n",
    "    %% Add styling\n",
    "    classDef arrow stroke-width:0px,fill-opacity:0.0 \n",
    "    classDef input stroke-width:0px,fill-opacity:0.3,fill:#22A699 \n",
    "    classDef output stroke-width:0px,fill-opacity:0.3,fill:#F2BE22 \n",
    "    classDef BrickNotTrainable stroke-width:0px,fill:#B56576 \n",
    "    classDef BrickTrainable stroke-width:0px,fill:#6D597A \n",
    "    classDef BrickMetricSingle stroke-width:0px,fill:#1450A3 \n",
    "    classDef BrickLoss stroke-width:0px,fill:#5C677D \n",
    "    \n",
    "    %% Add legends\n",
    "    subgraph Legends\n",
    "        input(input):::input\n",
    "        output(output):::output\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Export as ONNX\n",
    "To export a brick collection as onnx we provide the `export_bricks_as_onnx`-function. \n",
    "\n",
    "Pass an example input (`named_input`) to trace a brick collection.\n",
    "Set `dynamic_batch_size=True` to support any batch size inputs and here we explicitly set `stage=Stage.EXPORT` - this is also \n",
    "the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchbricks.brick_utils import export_bricks_as_onnx\n",
    "path_build = Path(\"build\")\n",
    "path_build.mkdir(exist_ok=True)\n",
    "path_onnx = path_build / \"readme_model.onnx\"\n",
    "\n",
    "export_bricks_as_onnx(path_onnx=path_onnx, \n",
    "                      brick_collection=brick_collection, \n",
    "                      named_inputs=named_inputs, \n",
    "                      dynamic_batch_size=True, \n",
    "                      stage=Stage.EXPORT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Bag of bricks - reusable bricks modules\n",
    "Note also in above example we use bag-of-bricks to import commonly used `nn.Module`s \n",
    "\n",
    "This includes a `Preprocessor`, `ImageClassifier` and `resnet_to_brick` to convert a torchvision resnet models to a backbone brick \n",
    "without a classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Training with pytorch-lightning trainer\n",
    "I like and love pytorch-lightning! We can avoid writing the easy-to-get-wrong training loop and validation/test scrips.\n",
    "\n",
    "Pytorch lightning creates logs, ensures training is done efficiently on any device (CPU, GPU, TPU), on multiple/distributed devices \n",
    "with reduced precision and much more.\n",
    "\n",
    "However, one issue I found myself having when wanting to extend my custom pytorch-lightning module (`LightningModule`) is that it forces an\n",
    "object oriented style with multiple levels of inheritance. This is not necessarily bad, but it makes it hard to reuse \n",
    "code across projects and generally makes the code complicated. \n",
    "\n",
    "With a brick collection you should rarely change or inherit your lightning module, instead you can inject the model, metrics and loss functions\n",
    "into a lightning module. Changes to preprocessor, backbone, necks, heads, metrics and losses are done on the outside\n",
    "and injected into the lightning module. \n",
    "\n",
    "Below is an example of how you could inject a brick collection with pytorch-lightning. \n",
    "\n",
    "We have created `LightningBrickCollection` ([available here](https://github.com/PeteHeine/torchbricks/blob/main/scripts/lightning_module.py)) \n",
    "as an example for you to use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from utils_testing.lightning_module import LightningBrickCollection\n",
    "from utils_testing.datamodule_cifar10 import CIFAR10DataModule\n",
    "\n",
    "experiment_name=\"CIFAR10\"\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "data_module = CIFAR10DataModule(data_dir='data', batch_size=5, num_workers=12, test_transforms=transform, train_transforms=transform)\n",
    "create_opimtizer_func = partial(torch.optim.SGD, lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "bricks_lightning_module = LightningBrickCollection(path_experiments=Path(\"build\") / \"experiments\",\n",
    "                                                   experiment_name=None,\n",
    "                                                   brick_collection=brick_collection,\n",
    "                                                   create_optimizers_func=create_opimtizer_func)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, limit_train_batches=2, limit_val_batches=2, limit_test_batches=2)\n",
    "# Train and test model by injecting 'bricks_lightning_module'\n",
    "# trainer.fit(bricks_lightning_module, datamodule=data_module)\n",
    "# trainer.test(bricks_lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Brick features: Pass all intermediate tensors to Brick\n",
    "By adding `'__all__'` to `input_names`, it is possible to access all tensors as a dictionary inside a brick module. \n",
    "For production code, this may not be the best option, but this feature can be valuable during an exploration phase or \n",
    "when doing some live debugging of a new model/module. \n",
    "\n",
    "We will demonstrate in code by introducing a (dummy) module `MyNewPostProcessor`.\n",
    "\n",
    "*Note: It is just a dummy class, don't worry to much about the actual implementation.*\n",
    "\n",
    "The important thing to notice is that `input_names = ['__all__']` is used for our `visualizer`-brick to\n",
    "pass all tensors as a dictionary as an argument in the forward call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNewPostProcessor(torch.nn.Module):\n",
    "    def forward(self, named_inputs: Dict[str, Any]):\n",
    "        ## Here `named_inputs` contains all intermediate tensors\n",
    "        assert \"raw\" in named_inputs\n",
    "        assert \"embedding\" in named_inputs\n",
    "        return named_inputs[\"embedding\"]\n",
    "\n",
    "\n",
    "bricks = {\n",
    "    'backbone': BrickTrainable(TinyModel(n_channels=3, n_features=10), input_names=['raw'], output_names=['embedding']),\n",
    "    'post_processor': BrickNotTrainable(MyNewPostProcessor(), input_names = ['__all__'], output_names=[\"postprocessed\"])\n",
    "}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "named_outputs = brick_collection(named_inputs={'raw': torch.rand((2, 3, 100, 200))}, stage=Stage.INFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Using Stage Inside Module\n",
    "By passing in `stage` in `input_names` it is possible to change the program flow. \n",
    "\n",
    "As demonstrated below want to alway resize the input to a specific size when the model is being exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(torch.nn.Module):\n",
    "    def forward(self, input_image: torch.Tensor, stage: Stage) -> str:\n",
    "        if stage in [Stage.EXPORT]:\n",
    "            input_image = torch.nn.functional.interpolate(input_image, size=(50,100))\n",
    "        return input_image/2\n",
    "\n",
    "\n",
    "brick_collection = BrickCollection({\n",
    "        \"preprocessor\": BrickNotTrainable(Preprocessor(), input_names=['raw', 'stage'], output_names=[\"processed\"])\n",
    "    })\n",
    "named_inputs = {'raw': torch.rand((2, 3, 100, 200))}\n",
    "named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.EXPORT)\n",
    "assert list(named_outputs[\"processed\"].shape[2:]) == [50, 100]\n",
    "\n",
    "named_outputs = brick_collection(named_inputs=named_inputs, stage=Stage.VALIDATION)\n",
    "assert list(named_outputs[\"processed\"].shape[2:]) == [100, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brick features: Visualizations in TorchBricks\n",
    "We provide `BrickPerImageVisualization` as base brick for doing visualizations in a brick collection. \n",
    "The advantage of brick-based visualization is that it can be bundled together with a specific task/head. \n",
    "\n",
    "Secondly, visualization/drawing functions typically operate on a single image and on non-`torch.Tensor` data types.\n",
    "E.g. Opencv/matplotlib uses `np.array` and pillow using `Image`. \n",
    "\n",
    "(Torchvision actually has functions to draw rectangles, key-points and segmentation masks directly on `torch.Tensor`s -\n",
    "but it still operates on a single image and it has no option for rendering text).\n",
    "\n",
    "The goal of `BrickPerImageVisualization` is to convert batched tensors/data to per image data in a desired format/datatype \n",
    "and pass it to a draw function. Look up the documentation of `BrickPerImageVisualization` to see all options.\n",
    "\n",
    "First we create a callable to do per image visualizations. It can be a simple function, but as demonstrated in below example, it \n",
    "can also be a callable class. \n",
    "\n",
    "The callable visualizes image classification predictions using pillow and requires two `np.array`s as input: \n",
    "`input_image` of shape [H, W, C] and `target_prediction` [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from torchbricks.tensor_conversions import float2uint8\n",
    "\n",
    "class VisualizeImageClassification:\n",
    "    def __init__(self, class_names: list, font_size: int = 50):\n",
    "        self.class_names = class_names\n",
    "        self.font = ImageFont.truetype('tests/data/font_ASMAN.TTF', size=font_size) \n",
    "\n",
    "    def __call__(self, input_image: np.ndarray, target_prediction: np.ndarray) -> Image.Image:\n",
    "        \"\"\"Draws image classification results\"\"\"\n",
    "        assert input_image.ndim == 3 # Converted to single image channel last numpy array [H, W, C]\n",
    "        image = Image.fromarray(float2uint8(input_image))\n",
    "        draw = ImageDraw.Draw(image) \n",
    "        draw.text((25, 25), text=self.class_names[target_prediction[0]], font = self.font) \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawing class `VisualizeImageClassification` is now passed to `BrickPerImageVisualization` and used in a brick collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchbricks.brick_visualizer import BrickPerImageVisualization\n",
    "bricks = {\n",
    "    'visualizer': BrickPerImageVisualization(callable=VisualizeImageClassification(class_names=[\"cat\", \"dog\"]),\n",
    "                                          input_names=[\"input_image\", \"target\"],\n",
    "                                          output_names=[\"visualization\"],\n",
    "                                          alive_stages=[Stage.INFERENCE])\n",
    "}\n",
    "\n",
    "batched_inputs = {'input_image': torch.zeros((2, 3, 100, 200)), 'target': torch.tensor([0, 1], dtype=torch.int64)}\n",
    "brick_collection = BrickCollection(bricks)\n",
    "outputs = brick_collection(named_inputs=batched_inputs, stage=Stage.INFERENCE)\n",
    "\n",
    "display(outputs[\"visualization\"][0],  outputs[\"visualization\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BrickPerImageProcessing` will by default convert a batch tensor of shape [B, C, H, W] to a channel last numpy image of shape [H, W, C]. \n",
    "This is the default behavior, and it allows us in the callable of `VisualizeImageClassification` to operate directly on numpy arrays. \n",
    "\n",
    "However for `BrickPerImageProcessing` a user has the option for unpacking batch data in a desired way as we will demonstrate in the \n",
    "next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a class that inherits `BrickPerImageVisualization` to create a brick for visualizing\n",
    "image classification `BrickVisualizeImageClassification`. The functionality is similar to above, but demonstrate \n",
    "other options of the `BrickPerImageVisualization` class. \n",
    "\n",
    "*It is important to note that `visualize_image_classification_pillow` is passed as a callable, and we do not override functionality of \n",
    "`BrickPerImageVisualization`. We only use it to simplify the constructor of `BrickVisualizeImageClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torchbricks.tensor_conversions import unpack_batched_tensor_to_pillow_images, function_composer, torch_to_numpy\n",
    "\n",
    "\n",
    "class BrickVisualizeImageClassification(BrickPerImageVisualization):\n",
    "    def __init__(self, input_image: str, target_name: str, class_names: List[str], output_name: str):\n",
    "        self.class_names = class_names\n",
    "        self.font = ImageFont.truetype('tests/data/font_ASMAN.TTF', 50) \n",
    "        super().__init__(callable=self.visualize_image_classification_pillow, input_names=[input_image, target_name], output_names=[output_name],\n",
    "                         unpack_functions_for_type={torch.Tensor: unpack_batched_tensor_to_pillow_images},\n",
    "                         unpack_functions_for_input_name={target_name: function_composer(torch_to_numpy, list)})\n",
    "\n",
    "    def visualize_image_classification_pillow(self, image: Image.Image, target_prediction: np.int64) -> Image.Image:\n",
    "        \"\"\"Draws image classification results\"\"\"\n",
    "        draw = ImageDraw.Draw(image) \n",
    "\n",
    "        draw.text((25, 25), text=self.class_names[target_prediction], font = self.font) \n",
    "        return image\n",
    "\n",
    "\n",
    "visualizer = BrickVisualizeImageClassification(input_image=\"input_image\", target_name=\"target\", class_names=[\"cat\", \"dog\"],\n",
    "                                               output_name=\"VisualizeImageClassification\")\n",
    "batched_inputs = {'input_image': torch.zeros((2, 3, 100, 200)), 'target': torch.tensor([0, 1], dtype=torch.int64)}\n",
    "visualizer(batched_inputs, stage=Stage.INFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not unlike before, the callable (here `visualize_image_classification_pillow`) accepts an `Image.Image` image and an `int64` value directly\n",
    "and we are not required to do conversions inside the drawing function. \n",
    "\n",
    "This can be achieved by using the two input arguments: \n",
    "- `unpack_functions_for_type: Dict[Type, Callable]` specifying how each type should be unpacked.\n",
    "  In above example we use `unpack_functions_for_type={torch.Tensor: unpack_batched_tensor_to_pillow_images}` to unpack all `torch.Tensor`s \n",
    "  of shape [B, 3, H, W] as pillow images.\n",
    "- `unpack_functions_for_input_name: Dict[str, Callable]` specifies how a specific input name should be unpacked. \n",
    "  In above example we use `unpack_functions_for_input_name={target_name: function_composer(torch_to_numpy, list)}` to unpack a \n",
    "  `torch.Tensor` of shape [B] to one int64 value per image. \n",
    "\n",
    "Specifying unpacking by input name (`unpack_functions_for_input_name`) will override the per type unpacking of `unpack_functions_for_type`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The main motivation:\n",
    "- Sharable models: Packing model parts, metrics, loss-functions and visualizations into a single recipe, makes the model more sharable to\n",
    "  other projects and supports sharing models for different use cases such as: Only inference, inference+visualizations and \n",
    "  training+metrics+losses.\n",
    "- Shareable Parts: The brick collection encourage users to decouples parts and making also each part more sharable. \n",
    "- Multiple tasks: Makes it easier to add and remove tasks. Each task can be expressed by model parts in a dictionary, \n",
    "  we can easily add/remove them to a brick collection. \n",
    "- By packing model modules, metrics, loss-functions and visualization into a single brick collection, we can more easily \n",
    "  inject it into your custom trainer and evaluation without doing per task/model modifications. \n",
    "- Your model is **not** required to only return logits. Some training frameworks expect you to only return logits - values that go into \n",
    "  your loss function. Then at inference/test/evaluation you need to do post processing or pass additional outputs to \n",
    "  calculate metrics, do visualizations and make prediction human interpretable. It encourage unclear control flow (if/else statements) \n",
    "  in the model that depends on model stage. \n",
    "- Using input and output names makes it easier to describe how parts are connected. Internally data is passed between bricks in a \n",
    "  dictionary of any type - making in flexible. But for each module, you can specific and add and check type hints for input and output \n",
    "  data to both improve readability and make it more production ready. \n",
    "- When I started making a framework suited for multiple tasks, I would passed dictionaries around to all modules and pull out tensors by\n",
    "  name in each module. Book keeping names and updating names was messy. \n",
    "  I also started using the typical backbone(encoder) / head(decoder) separation... But some heads may share a common neck. \n",
    "  The decoder might also take different inputs and\n",
    "  split into different representation and merge again... Also to avoid code duplication, I ended up during \n",
    "  multiple layers of inheritance for the decoder, making reuse bad and generally everything became too complicated and a new task would \n",
    "  require me to refactor the whole concept. Yes, it was probably not a super great attempt either, but it made me realize it should be \n",
    "  easier to make a new task and it should be easier to reuse parts. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "## What are we missing?\n",
    "- [x] Saving-loading brick collections\n",
    "  - The user is able to define a model in code and from config (From config require it as an argument in the init-function?)\n",
    "  - I have decided to only provide a \"path_weights\" in each brick collection. Each brick collection will load weights if the weights exists in a given folder. wrong path -> error, missing path -> warning of module (warning), warning in case a file is not used? \n",
    "  - Currently, this option supports training sub-node from scratch by removing the weight file from the folder.\n",
    "  -  [ ] Create an example in README\n",
    "- [ ] Move parts generic parts from model-trainer to torch-bricks\n",
    "- [ ] A user can pass in both stage as a str and as an enum. (It is always a string internally). String makes it easier to jit trace and we\n",
    "      a user can create self-defined stages. \n",
    "- [ ] Demonstrate model configuration with hydra in this document\n",
    "- [ ] Add stage as an internal state and not in the forward pass:\n",
    "  - Minor Pros: Tracing (to get onnx model) requires 'torch.Tensors' only as input - we avoid making an adapter class. \n",
    "  - Minor Cons: State gets hidden away - implicit instead of explicit.\n",
    "  - Minor Pros: Similar to eval/training in pytorch\n",
    "  - Minor Pros: The forward call does not require the user to always pass the stage - less typing.\n",
    "Update version!\n",
    "\n",
    "- [ ] Make common Visualizations with pillow - not opencv to not blow up the required dependencies. ImageClassification, Segmentation, ObjectDetection\n",
    "  - [ ] VideoModule to store data as a video\n",
    "  - [ ] DisplayModule to show data\n",
    "- [ ] Consider caching unpacked data for `PerImageVisualizer`\n",
    "- [ ] Multiple named tensors caching module. \n",
    "- [ ] Use pymy, pyright or pyre to do static code checks. \n",
    "- [ ] Collection of helper modules. Preprocessors, Backbones, Necks/Upsamplers, ImageClassification, SemanticSegmentation, ObjectDetection\n",
    "  - [ ] Make common brick collections: BricksImageClassification, BricksSegmentation, BricksPointDetection, BricksObjectDetection\n",
    "- [ ] Support preparing data in the dataloader?\n",
    "- [ ] Support torch.jit.scripting? \n",
    "\n",
    "## How does it really work?\n",
    "????\n",
    "\n",
    "\n",
    "\n",
    "## Development\n",
    "\n",
    "Read the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n",
    "\n",
    "### Install\n",
    "\n",
    "    conda create --name torchbricks --file conda-linux-64.lock\n",
    "    conda activate torchbricks\n",
    "    poetry install\n",
    "\n",
    "### Activating the environment\n",
    "\n",
    "    conda activate torchbricks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
